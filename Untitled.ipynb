{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "palestinian-amazon",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-705ff59fe37f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# For example, here's several helpful packages to load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Recommending Movies On MovieLens\n",
    "# In this notebook we are going to focus on content based filtering with features from the movies such as the overview, the crew and additional keyword features\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Importing Libraries\n",
    "\n",
    "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-06-11T17:39:03.263675Z\",\"iopub.execute_input\":\"2021-06-11T17:39:03.264411Z\",\"iopub.status.idle\":\"2021-06-11T17:39:03.288786Z\",\"shell.execute_reply.started\":\"2021-06-11T17:39:03.264215Z\",\"shell.execute_reply\":\"2021-06-11T17:39:03.286582Z\"}}\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/demo'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Inspecting Datasets\n",
    "# In this part we have 5 datasets, we are only going to focus on 3 of them, which contain movie metadata (aka features of movies) along with some additional features such as keywords and credits.\n",
    "\n",
    "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-06-11T17:39:03.293958Z\",\"iopub.execute_input\":\"2021-06-11T17:39:03.294459Z\"}}\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "links = pd.read_csv(\"links.csv\")\n",
    "movies_meta = pd.read_csv(\"movies_metadata.csv\")\n",
    "keywords = pd.read_csv(\"keywords.csv\")\n",
    "credits = pd.read_csv(\"credits.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Ratings\n",
    "# Contains ratings that have been given by users for movies in certain timestamps\n",
    "\n",
    "# %% [code]\n",
    "ratings.head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Links\n",
    "# This file contains certain links for movies, their IDs on both IMDB and TMDB\n",
    "\n",
    "# %% [code]\n",
    "links.head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Movie Metadata\n",
    "# The main dataset which has a number of features such as budget, genres, original language, overview and runtime.\n",
    "\n",
    "# %% [code]\n",
    "movies_meta.head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Credits\n",
    "# An additional feature table that contains useful information about the crews of movies, its data can be combined with movies metadata as we will see later.\n",
    "\n",
    "# %% [code]\n",
    "credits.head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Keywords\n",
    "# An additional feature table that contains useful information about what keywords does a movie have, its data can be combined with movies metadata also.\n",
    "\n",
    "# %% [code]\n",
    "keywords.head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Preprocessing\n",
    "# In this part we are going to do some data preprocessing to try to gain insight from the data and make sure the results make sense, \n",
    "\n",
    "# %% [markdown]\n",
    "# ### Limiting the Dataset to Popular Movies\n",
    "# In this part we limit the dataset to the highest 90% popular movies, so that the number of raters doesn't affect the average rating. If for example a movie had an average rating of 7 with 50 votes, it would be certainly better than another movie with the same average rating but with only 3 votes. \n",
    "\n",
    "# %% [code]\n",
    "# Calculate mean of vote average column\n",
    "C = movies_meta['vote_average'].mean()\n",
    "print(C)\n",
    "\n",
    "# %% [code]\n",
    "# Calculate the minimum number of votes required to be in the chart, m\n",
    "m = movies_meta['vote_count'].quantile(0.90)\n",
    "print(m)\n",
    "\n",
    "# %% [code]\n",
    "popular_movies = movies_meta.loc[movies_meta['vote_count'] >= m, :] \n",
    "popular_movies.shape\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Weighted Average Rating for Each Movie\n",
    "# Also one of the good ways to normalize ratings is to average them by the average rating of the whole dataset, and the number of voters for movies which have more than or equal to 160 voters\n",
    "\n",
    "# %% [code]\n",
    "# Function that computes the weighted rating of each movie\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    # Calculation based on the IMDB formula\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "# %% [code]\n",
    "popular_movies['score'] = popular_movies.apply(weighted_rating, axis = 'columns')\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Top Rated Movies\n",
    "# Then we try to gain an important insight about which movies are the most popular and have the highest average rating, we are now looking at a list that pretty makes sense compared to the situation where movies with 5-10 reviews are treated the same as movies with +160 reviews\n",
    "\n",
    "# %% [code]\n",
    "popular_movies_srt = popular_movies.sort_values('score', ascending = False)\n",
    "popular_movies_srt[['title', 'vote_count', 'vote_average', 'score']].head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Overview Based Recommendations\n",
    "# First we are going to look at how to derive features from a simple attribute, which is the overview of the movie, that describes how the movie will go and probably some teaser words\n",
    "\n",
    "# %% [code]\n",
    "metadata = popular_movies.copy().reset_index()\n",
    "\n",
    "# %% [code]\n",
    "metadata['overview'].head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### TF-IDF Vectorization\n",
    "# Then we are going to calculate the text extracted from overviews to be able to be processed in a different manner (\"of numbers\"), so what basically TF-IDF means is that it is Term-Frequency - Inverse Document Frequency, its main function is to represent words with frequencies, with a little tweak (IDF) that doesn't let the model be biased towards the most occurring words in the overview.\n",
    "\n",
    "# %% [code]\n",
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "metadata['overview'] = metadata['overview'].fillna('')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(metadata['overview'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Measuring Similarity Between Content\n",
    "# Then we are going to measure similarity between extracted features for each movie and store the similarity values in a matrix that will be used later, and based on that we will try to recommend movies that are \"similar\" (based on the overview) with other movies.\n",
    "\n",
    "# %% [code]\n",
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# %% [code]\n",
    "cosine_sim.shape\n",
    "\n",
    "# %% [code]\n",
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "# %% [code]\n",
    "indices[:10]\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Top Similar Movies Given a Movie\n",
    "# As promised, we are going to get similar movies based on the content of our movie, (in this case, the content is the overview of the movie).\n",
    "\n",
    "# %% [code]\n",
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "\n",
    "# %% [code]\n",
    "get_recommendations('The Dark Knight Rises')\n",
    "\n",
    "# %% [code]\n",
    "get_recommendations('The Godfather')\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Using Additional Features (Crew and Keywords)\n",
    "# In this section we are going to use the additional tables that we talked about earlier, and try to do the same thing as we did in the overview, but this time we are going to use a Bag of Words for evaluating similarity.\n",
    "\n",
    "# %% [code]\n",
    "# Remove rows with bad IDs.\n",
    "# metadata = metadata.drop([19730, 29503, 35587])\n",
    "\n",
    "# Convert IDs to int. Required for merging\n",
    "keywords['id'] = keywords['id'].astype('int')\n",
    "credits['id'] = credits['id'].astype('int')\n",
    "metadata['id'] = metadata['id'].astype('int')\n",
    "\n",
    "# Merge keywords and credits into your main metadata dataframe\n",
    "metadata = metadata.merge(credits, on='id')\n",
    "metadata = metadata.merge(keywords, on='id')\n",
    "\n",
    "# %% [code]\n",
    "metadata.head(5)\n",
    "\n",
    "# %% [code]\n",
    "# Parse the stringified features into their corresponding python objects\n",
    "from ast import literal_eval\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(literal_eval)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Director's Name\n",
    "# Director's Name is one of the important factors in recommending movies, each director has his own unique style that is repeated with different variations for each and every movie, so it is easy to distinguish Quentin Tarantino's movies to be somehow gorish, some other movies like Alfred Hitchcock's movies to be centered more around horror.\n",
    "\n",
    "# %% [code]\n",
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Text Extraction from Additional Features\n",
    "# To decrease the level of sparsity of the dataset, as there are many movies with different numbers of recorded Crew and keywords, we have chosen to take the first 3 words out of each movie's crew and keywords, to insure data consistency. We could have done it using several other ways such as reducing the number of words to n_components using Principal Component Analysis or other dimensionality reduction techniques.\n",
    "\n",
    "# %% [code]\n",
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "\n",
    "    #Return empty list in case of missing/malformed data\n",
    "    return []\n",
    "\n",
    "# %% [code]\n",
    "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "metadata['director'] = metadata['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(get_list)\n",
    "\n",
    "# %% [code]\n",
    "# Print the new features of the first 3 films\n",
    "metadata[['title', 'cast', 'director', 'keywords', 'genres']].head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Text Cleaning\n",
    "\n",
    "# %% [code]\n",
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "# %% [code]\n",
    "# Apply clean_data function to your features.\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(clean_data)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Feature Aggregation\n",
    "# In this part we are going to join the features in a clear document (same as overview, but larger), to be able to represent the text as a whole.\n",
    "\n",
    "# %% [code]\n",
    "def create_soup(x):\n",
    "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
    "\n",
    "# %% [code]\n",
    "# Create a new soup feature\n",
    "metadata['soup'] = metadata.apply(create_soup, axis=1)\n",
    "\n",
    "# %% [code]\n",
    "metadata[['soup']].head(5)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Count Vectorization\n",
    "\n",
    "# %% [markdown]\n",
    "# In this part we are going to use another type of text representation to be able to derive features from keywords, combined with actors and directors. We use count vectorization as sometimes the directors, for example, take much more weight than actors in movie recommendation, that's why we want it to stand out more.\n",
    "\n",
    "# %% [code]\n",
    "# Import CountVectorizer and create the count matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(metadata['soup'])\n",
    "\n",
    "# %% [code]\n",
    "count_matrix.shape\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Measuring Similarity Between Content \n",
    "# Then we are going to measure similarity between extracted features for each movie and store the similarity values in a matrix that will be used later, and based on that we will try to recommend movies that are \"similar\" (based on crew and keywords) with other movies.\n",
    "\n",
    "# %% [code]\n",
    "# Compute the Cosine Similarity matrix based on the count_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "# %% [code]\n",
    "# Reset index of your main DataFrame and construct reverse mapping as before\n",
    "metadata = metadata.reset_index()\n",
    "indices = pd.Series(metadata.index, index=metadata['title'])\n",
    "\n",
    "# %% [code]\n",
    "get_recommendations('The Dark Knight Rises', cosine_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-assistant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-emission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
